<!doctype html><!-- This site was created with Wowchemy. https://www.wowchemy.com --><!-- Last Published: June 8, 2025 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload="this.media='all'"><link rel=stylesheet href=/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload="this.media='all'"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload="this.media='all'"><link rel=stylesheet href=/css/wowchemy.e5d7adca760216d3b7e28ea434e81f6f.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload="this.media='all'" disabled><meta name=author content="Yosuke Shinya"><meta name=description content="Yosuke Shinya is a research engineer in computer vision and deep learning."><link rel=alternate hreflang=en-us href=https://shinya7y.github.io/><link rel=canonical href=https://shinya7y.github.io/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_huf7c422d5258ab66a3952fb2c8d0b75fd_55535_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_huf7c422d5258ab66a3952fb2c8d0b75fd_55535_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@shinya7y"><meta property="twitter:creator" content="@shinya7y"><meta property="twitter:image" content="https://shinya7y.github.io/media/icon_huf7c422d5258ab66a3952fb2c8d0b75fd_55535_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="Yosuke Shinya"><meta property="og:url" content="https://shinya7y.github.io/"><meta property="og:title" content="Yosuke Shinya"><meta property="og:description" content="Yosuke Shinya is a research engineer in computer vision and deep learning."><meta property="og:image" content="https://shinya7y.github.io/media/icon_huf7c422d5258ab66a3952fb2c8d0b75fd_55535_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-12-13T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://shinya7y.github.io/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://shinya7y.github.io/"}</script><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Yosuke Shinya"><title>Yosuke Shinya</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper><script src=/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Yosuke Shinya</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Yosuke Shinya</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#publications data-target=#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#awards data-target=#awards><span>Awards</span></a></li><li class=nav-item><a class=nav-link href=/#competitions data-target=#competitions><span>Competitions</span></a></li><li class=nav-item><a class=nav-link href=/#misc data-target=#misc><span>Misc</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=about class="home-section wg-about"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-square" width=270 height=270 src=/authors/admin/avatar_huf7c422d5258ab66a3952fb2c8d0b75fd_55535_270x270_fill_lanczos_center_3.png alt="Yosuke Shinya"><div class=portrait-title><h2>Yosuke Shinya</h2></div><ul class=network-icon aria-hidden=true><li><a href=https://twitter.com/shinya7y target=_blank rel=noopener aria-label=twitter><i class="fab fa-twitter big-icon"></i></a></li><li><a href="https://scholar.google.com/citations?user=oGtEodQAAAAJ" target=_blank rel=noopener aria-label=google-scholar><i class="ai ai-google-scholar big-icon"></i></a></li><li><a href=https://github.com/shinya7y target=_blank rel=noopener aria-label=github><i class="fab fa-github big-icon"></i></a></li><li><a href=https://www.linkedin.com/in/yosukeshinya target=_blank rel=noopener aria-label=linkedin><i class="fab fa-linkedin big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>Biography</h1><div class=article-style><p>Yosuke Shinya is a research engineer in computer vision and deep learning, especially in object detection and image generation.
He is a senior researcher at SenseTime Japan.
He has received
the Award of the Minister of State for Science and Technology Policy (at the 50th Japan Student Science Award),
OpenMMLab Contributor of the Year Award 2021,
and
Papers with Code Top Contributor Award.</p><p>進矢陽介：物体検出や画像生成を中心としたコンピュータビジョンの研究開発に従事。
2014年東京大学大学院情報理工学系研究科修士課程修了。
三菱電機株式会社、株式会社デンソーを経て、2024年に株式会社センスタイムジャパンに入社。
主な受賞歴に、第50回日本学生科学賞 科学技術政策担当大臣賞、OpenMMLab Contributor of the Year Award 2021、Papers with Code Top Contributor Award。</p><!--

  <i class="fas fa-download  pr-1 fa-fw"></i> Download my 

<a href="/uploads/resume.pdf" target="_blank">resumé</a>
.
--></div><div class=row><div class=col-md-4><div class=section-subheading>Interests</div><ul class="ul-interests mb-0"><li>Computer Vision</li><li>Deep Learning</li><li>Computer Graphics</li><li>Augmented Reality</li></ul></div><div class=col-md-8><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0"><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>Master of Information Science and Technology, 2014</p><p class=institution>The University of Tokyo</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>Bachelor of Engineering, 2012</p><p class=institution>The University of Tokyo</p></div></li></ul></div></div></div></div></div></section><section id=experience class="home-section wg-experience"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Experience</h1></div><div class="col-12 col-lg-8"><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border exp-fill">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="section-subheading card-title exp-title text-muted my-0">Senior Researcher</div><div class="section-subheading card-title exp-company text-muted my-0">SenseTime Japan</div><div class="text-muted exp-meta">Jun 2024 –
Present
<span class=middot-divider></span>
<span>Tokyo, Japan</span></div><div class=card-text>Conducting research and development on computer vision.</div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="section-subheading card-title exp-title text-muted my-0">Research Engineer, Project Assistant Manager</div><div class="section-subheading card-title exp-company text-muted my-0">DENSO</div><div class="text-muted exp-meta">Dec 2015 –
May 2024
<span class=middot-divider></span>
<span>Tokyo, Japan</span></div><div class=card-text><p>Conducted research and development on computer vision for self-driving cars and ADAS.</p><ul><li>Object detection</li><li>Efficient deep learning (model compression, transfer learning)</li><li>Image generation (GANs, diffusion models)</li><li>3D model generation</li><li>Technology trends survey</li></ul></div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="section-subheading card-title exp-title text-muted my-0">Research Engineer</div><div class="section-subheading card-title exp-company text-muted my-0">Mitsubishi Electric</div><div class="text-muted exp-meta">Apr 2014 –
Nov 2015
<span class=middot-divider></span>
<span>Hyogo, Japan</span></div><div class=card-text><p>Developed software for driver monitoring system.</p><ul><li>Driver&rsquo;s cognitive distraction detection using deep neural networks</li><li>Occupant detection</li><li>AR HUD (Augmented Reality Head-Up Display)</li></ul></div></div></div></div></div></div></div></div></section><section id=publications class="home-section wg-collection"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Publications</h1><p class=mt-1><a href=./publication/>Full list</a><br><a href="https://scholar.google.com/citations?user=oGtEodQAAAAJ" target=_blank rel=noopener>Google Scholar</a></p></div><div class="col-12 col-lg-8"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yosuke Shinya</span>, <span>Kenichi Yoneji</span>, <span>Akihiro Tsukada</span>, <span>Tatsuya Harada</span>.</span>
<a href=/publication/3dlighter-shinya-sa-2023/>3D Lighter: Learning to Generate Emissive Textures</a>.
<em>SIGGRAPH Asia Posters</em>,
2023.<div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/3dlighter-shinya-sa-2023/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3610542.3626153&file=3dlighter_sa2023_poster.pdf" target=_blank rel=noopener>Poster</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://dl.acm.org/doi/abs/10.1145/3610542.3626153 target=_blank rel=noopener>ACM</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yosuke Shinya</span>.</span>
<a href=/publication/bandre-shinya-mva-2023/>BandRe: Rethinking Band-Pass Filters for Scale-Wise Object Detection Evaluation</a>.
<em>International Conference on Machine Vision and Applications (MVA)</em>,
2023.
Oral presentation, Honorable Mention Solution Award in Small Object Detection Challenge for Spotting Birds.<div class=btn-links><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/bandre-shinya-mva-2023/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2307.11748 target=_blank rel=noopener>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/shinya7y/UniverseNet target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/shinya7y/shinya7y.github.io/releases/download/v5.7.1/bandre-shinya-mva-2023-slides.pdf target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ieeexplore.ieee.org/document/10216132 target=_blank rel=noopener>IEEE</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yosuke Shinya</span>.</span>
<a href=/publication/usb-shinya-bmvc-2022/>USB: Universal-Scale Object Detection Benchmark</a>.
<em>British Machine Vision Conference (BMVC)</em>,
2022.<div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://bmvc2022.mpi-inf.mpg.de/0261.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/usb-shinya-bmvc-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2103.14027 target=_blank rel=noopener>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/shinya7y/UniverseNet target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/shinya7y/shinya7y.github.io/releases/download/v5.7.1/usb-shinya-bmvc-2022-slides.pdf target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://bmvc2022.mpi-inf.mpg.de/0261_poster.pdf target=_blank rel=noopener>Poster</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Laurent Dillard</span>, <span>Yosuke Shinya</span>, <span>Taiji Suzuki</span>.</span>
<a href=/publication/momasp-dillard-bmvc-2020/>Domain Adaptation Regularization for Spectral Pruning</a>.
<em>British Machine Vision Conference (BMVC)</em>,
2020.
Acceptance rate: 29.1%.<div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.bmvc2020-conference.com/assets/papers/0453.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/momasp-dillard-bmvc-2020/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1912.11853 target=_blank rel=noopener>arXiv</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yosuke Shinya</span>, <span>Edgar Simo-Serra</span>, <span>Taiji Suzuki</span>.</span>
<a href=/publication/intrinsicdet-shinya-iccvw-2019/>Understanding the Effects of Pre-Training for Object Detectors via Eigenspectrum</a>.
<em>International Conference on Computer Vision Workshops (ICCVW)</em>,
2019.
Oral presentation, Acceptance rate: 9%, Best Paper Award Nominee.<div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://esslab.jp/~ess/publications/ShinyaICCVW2019.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/intrinsicdet-shinya-iccvw-2019/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1909.04021 target=_blank rel=noopener>arXiv</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://neuralarchitects.org/slides/shinya-slides.pdf target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ieeexplore.ieee.org/document/9022267 target=_blank rel=noopener>IEEE</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_ICCVW_2019/html/NeurArch/Shinya_Understanding_the_Effects_of_Pre-Training_for_Object_Detectors_via_Eigenspectrum_ICCVW_2019_paper.html target=_blank rel=noopener>CVF</a></div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Hideomi Tsunakawa</span>, <span>Yoshitaka Kameya</span>, <span>Hanju Lee</span>, <span>Yosuke Shinya</span>, <span>Naoki Mitsumoto</span>.</span>
<a href=/publication/crp-tsunakawa-ijcnn-2019/>Contrastive Relevance Propagation for Interpreting Predictions by a Single-Shot Object Detector</a>.
<em>International Joint Conference on Neural Networks (IJCNN)</em>,
2019.
Oral presentation, Acceptance rate: 36.4%.<div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.researchgate.net/profile/Yoshitaka-Kameya/publication/332344761_Contrastive_Relevance_Propagation_for_Interpreting_Predictions_by_a_Single-Shot_Object_Detector/links/5db39aa1a6fdccc99d9d085b/Contrastive-Relevance-Propagation-for-Interpreting-Predictions-by-a-Single-Shot-Object-Detector.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/crp-tsunakawa-ijcnn-2019/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://rjida.meijo-u.ac.jp/kameya/publication/Tsunakawa-IJCNN19-slides.pdf target=_blank rel=noopener>Slides</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ieeexplore.ieee.org/document/8851770 target=_blank rel=noopener>IEEE</a></div></div></div></div></div></section><section id=awards class="home-section wg-markdown"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Awards</h1></div><div class="col-12 col-lg-8"><ul><li>President&rsquo;s Commendation, Active Challenge Award<div class="text-muted exp-meta">DENSO, Dec 2022</div></li><li>CTO&rsquo;s Commendation, Active Challenge Award<div class="text-muted exp-meta">DENSO, Dec 2022</div></li><li>Papers with Code Top Contributor Award<div class="text-muted exp-meta">Meta, Mar 2022</div></li><li>OpenMMLab Contributor of the Year Award 2021<div class="text-muted exp-meta">OpenMMLab, Jan 2022</div></li><li>The 56th Acacia Award<div class="text-muted exp-meta">Hiroshima University Senior High School, Jan 2007</div></li><li>The Award of the Minister of State for Science and Technology Policy
at the 50th Japan Student Science Award<div class="text-muted exp-meta">Minister of Japan, Dec 2006</div></li><li>Outstanding Reviewer<div class="text-muted exp-meta">NeurIPS Datasets and Benchmarks Track 2022, BMVC 2022</div></li></ul><!--
https://neurips.cc/Conferences/2022/DatasetBenchmarkProgramCommittee
https://bmvc2022.org/people/reviewers/
--></div></div></div></section><section id=competitions class="home-section wg-markdown"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Competitions</h1></div><div class="col-12 col-lg-8"><ul><li>COCO test-dev AP 54.1% (to our knowledge, state-of-the-art accuracy by training within 24 epochs, as of November 14, 2020).</li><li>1st place accuracy in 2nd AI Edge Contest.</li><li>Top single-stage detector in Waymo Open Dataset challenge 2D detection track, Workshop on Scalability in Autonomous Driving, CVPR 2020.</li><li>1st place in NightOwls Detection Challenge 2020 object detection from single frame track, Workshop on Scalability in Autonomous Driving, CVPR 2020. Oral presentation.</li></ul></div></div></div></section><section id=misc class="home-section wg-markdown"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Misc</h1></div><div class="col-12 col-lg-8"><h2 id=book-chapter>Book chapter</h2><ul><li>進矢陽介. イマドキノ物体検出. コンピュータビジョン最前線 Summer 2025. 共立出版, 2025.</li></ul><h2 id=japanese-translation>Japanese translation</h2><ul><li><a href=https://shinya7y.github.io/note/detection/ObjectDetectionSurvey_jp.pdf target=_blank rel=noopener>Deep Learning for Generic Object Detection: A Survey (IJCV 2019) 日本語訳</a></li></ul><h2 id=survey-and-technical-explanation>Survey and technical explanation</h2><ul><li><a href=https://confit.atlas.jp/guide/event/ssii2024/static/special_project_tech_map target=_blank rel=noopener>Object detection technology map (SSII 2024)</a></li><li><a href=https://www.slideshare.net/YosukeShinya/presentations target=_blank rel=noopener>SlideShare</a></li><li><a href=https://speakerdeck.com/shinya7y/ target=_blank rel=noopener>Speaker Deck</a></li></ul><h2 id=unofficial-demo>Unofficial demo</h2><ul><li><a href=https://shinya7y.github.io/playground/shadesketch/ target=_blank rel=noopener>Learning to Shadow Hand-drawn Sketches (CVPR 2020)</a></li><li><a href=https://shinya7y.github.io/playground/envmaps.html target=_blank rel=noopener>Deep Normal Estimation for Automatic Shading of Hand-Drawn Characters (ECCVW 2018) with environment mapping</a></li></ul><h2 id=academic-services>Academic services</h2><ul><li>Program committee and organizer of organized session (SSII 2025)</li><li>Reviewer (NeurIPS Datasets and Benchmarks Track 2022, BMVC 2020, 2022)</li></ul><h2 id=membership>Membership</h2><ul><li>Computer Vision Foundation (CVF)</li><li>Survey member of cvpaper.challenge</li></ul></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 Yosuke Shinya.</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.e66e385e2f1df861699d60acd7a9c670.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.0c1be879804fa6ae06a8f4c131cdbf54.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>